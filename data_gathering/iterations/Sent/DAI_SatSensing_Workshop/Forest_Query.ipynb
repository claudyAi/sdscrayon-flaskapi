{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sb1jcfkRAqO_"
   },
   "source": [
    "# World Airport Imagery Retrieval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the Aviation Fanatic dataset to query the Sentinel2 API using geometries constructed using Shapely and constructs the URLS needed for each airport image and stores it into a JSON file which can be referenced later as metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8_KhTV-VBWnb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\earth\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import folium\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from sentinelsat import SentinelAPI\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from shapely.geometry import Polygon\n",
    "import fiona\n",
    "from pyproj import Proj, CRS,transform\n",
    "from datetime import datetime\n",
    "import pygc\n",
    "from io import StringIO\n",
    "import shutil\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import os\n",
    "import rasterio as rio\n",
    "from rasterio.mask import mask\n",
    "from osgeo import gdal\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_datafile_path = \"estingAustralia.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking all airports based on IATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treecoords = pd.read_csv(main_datafile_path)\n",
    "treecoords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading current file and removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = treecoords.loc[0:0]\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sitelist = treecoords['site']\n",
    "# lat = list(treecoords['lat'])\n",
    "# lon = list(treecoords['long'])\n",
    "# projectlist = list(treecoords[\"project\"])\n",
    "\n",
    "sitelist = batch['site']\n",
    "lat = list(batch['lat'])\n",
    "lon = list(batch['long'])\n",
    "projectlist = list(batch[\"project\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________ END OF DATA WRANGLING ______________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have only played in getting the right data. This step could be much shorter and with less data files if your dataset is already clean. In short, you need a dataset with the names tagged to their right coordinates (lat-lon) and you can skip this entire above section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geo Boundary Construction around the airport "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below step we write a function that takes a coordinate point and draws a square with the specified dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latLonBoxByWandH(lat,lon,ew_width,ns_height):\n",
    "    lats, lons = [], []\n",
    "    #distance in m, az (in deg), lat (in deg), long (in deg)\n",
    "\n",
    "    res = pygc.great_circle(distance=ew_width/2, azimuth=90, latitude=lat, longitude=lon)\n",
    "    lat, lon = res['latitude'], res['longitude']\n",
    "\n",
    "    res = pygc.great_circle(distance=ns_height/2, azimuth=180, latitude=lat, longitude=lon)\n",
    "    lat, lon = res['latitude'], res['longitude']\n",
    "    lats.append(lat), lons.append(lon)\n",
    "\n",
    "    res = pygc.great_circle(distance=ew_width, azimuth=270, latitude=lat, longitude=lon)\n",
    "    lat, lon = res['latitude'], res['longitude']\n",
    "    lats.append(lat), lons.append(lon)\n",
    "\n",
    "    res = pygc.great_circle(distance=ns_height, azimuth=0, latitude=lat, longitude=lon)\n",
    "    lat, lon = res['latitude'], res['longitude']\n",
    "    lats.append(lat), lons.append(lon)\n",
    "\n",
    "    res = pygc.great_circle(distance=ew_width, azimuth=90, latitude=lat, longitude=lon)\n",
    "    lat, lon = res['latitude'], res['longitude']\n",
    "    lats.append(lat), lons.append(lon)\n",
    "    \n",
    "    return {'lats':lats,'lons':lons}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Boundary parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below numbers are the side lengths of the square in m. The length also decides the size of the final image as Sentinel2 provides images at a resolution of 10/pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lengths in m\n",
    "ew_width = 2000\n",
    "ns_height = 2000\n",
    "size = int(ew_width/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loc_lon,loc_lat\n",
    "loc_lat = np.asfarray(lat,float)\n",
    "loc_lon = np.asfarray(lon,float)\n",
    "len(loc_lat),len(loc_lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing Geo Boundaries from coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons,footprint =[], []\n",
    "\n",
    "for lat, lon, code in tqdm(zip(loc_lat,loc_lon, sitelist), total = len(loc_lat)):\n",
    "\n",
    "    box = latLonBoxByWandH(lat,lon,ew_width,ns_height)\n",
    "    \n",
    "    polygon_geom = Polygon(zip(box['lons'], box['lats']))\n",
    "    footprint.append(polygon_geom)\n",
    "    crs = CRS('epsg:4326')\n",
    "    polygon = gpd.GeoDataFrame(index=[0], crs=crs, geometry=[polygon_geom])  \n",
    "    \n",
    "    # Save polygon to disk for later use\n",
    "    with open(\"copingmarkers.shp\", \"wb\") as poly_file:\n",
    "        pickle.dump(polygon, poly_file, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    # polygon.to_file(\"copingmarkers.shp\")\n",
    "\n",
    "    polygons.append(polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the Basemap with all Airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Proj(\"epsg:4326\")\n",
    "\n",
    "m = folium.Map([loc_lat[0],loc_lon[0]], zoom_start=12,tiles = \"https://{s}.basemaps.cartocdn.com/dark_nolabels/{z}/{x}/{y}.png\",\n",
    "attr = '&copy; <a href=\"https://www.openstreetmap.org/copyright\">OpenStreetMap</a> contributors &copy; <a href=\"https://carto.com/attributions\">CARTO</a>')\n",
    "for polygon in polygons:\n",
    "    folium.GeoJson(polygon).add_to(m)\n",
    "    \n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally save the Basemap to HTML file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m.save('abovemap.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Query parameters, start and end conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Register for an account and replace XXX with your credentials\n",
    "\n",
    "https://scihub.copernicus.eu/dhus/#/home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rs5TUxLbBCEr"
   },
   "outputs": [],
   "source": [
    "user = 'fries' \n",
    "password = 'astorea4358' \n",
    "\n",
    "api = SentinelAPI(user, password, 'https://scihub.copernicus.eu/dhus')\n",
    "start = '20220101'\n",
    "end = '20220601'\n",
    "cloudperc = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apiQuery(iatalist,geometry,dateStart,dateEnd,cloud):\n",
    "    query = api.query(geometry,\n",
    "                # date = (str(getDateStamp(i)[0]),str(getDateStamp(i)[1])),\n",
    "                # date = (\"NOW-400DAYS\", \"NOW\"),\n",
    "                date = (str(dateStart), str(dateEnd)),\n",
    "                platformname = 'Sentinel-2',\n",
    "                processinglevel = 'Level-2A',\n",
    "                area_relation = ('Contains'),\n",
    "                cloudcoverpercentage = (0,cloud))\n",
    "    return query\n",
    "    #print(ia, len(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying SentinelAPI based on the constructed geo boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "apiq = []\n",
    "# for ia,boundary in tqdm(zip(iatalist,footprint),total=len(iatalist)):\n",
    "for ia,boundary in tqdm(zip(sitelist,footprint), total=len(sitelist)):\n",
    "    cloudperc = 80\n",
    "    queried = apiQuery(ia,boundary,start,end,cloudperc)\n",
    "    # while len(queried) == 0:\n",
    "    #     cloudperc +=10\n",
    "    #     minperc = min(cloudperc, 100)\n",
    "    #     # print(ia, minperc)\n",
    "    #     queried = apiQuery(ia,boundary,start,end,minperc)\n",
    "    apiq.append(queried)\n",
    "#     print(ia,len(queried))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "colab_type": "code",
    "id": "1svWVHFUCsjL",
    "outputId": "6f56c155-631a-4451-902e-f35d0c6c65fc"
   },
   "outputs": [],
   "source": [
    "products_list, products_list_sorted,images,titlelist,bestlist,datestamp,cloudiness =[],[], [], [], [] , [],[]\n",
    "for products in apiq:\n",
    "        products_list.append(api.to_geodataframe(products))\n",
    "        \n",
    "#Sorting the list of products within our array of locations for minimum cloudcover\n",
    "for products in tqdm(products_list, total = len(sitelist)):\n",
    "    try:\n",
    "        products_list_sorted.append(products.sort_values(['cloudcoverpercentage'],ascending = [True]))\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# for i in range(0,len(products_list_sorted)):\n",
    "#         images.append(products_list_sorted[i].head(1))\n",
    "\n",
    "for i in range(0,len(images)):\n",
    "    titlelist.append(products_list_sorted[i].title[0])\n",
    "    bestlist.append(products_list_sorted[i].uuid[0])\n",
    "    d = products_list_sorted[i].beginposition[0].date().strftime(\"%Y%m%d\")\n",
    "    c = products_list_sorted[i].cloudcoverpercentage\n",
    "    datestamp.append(d)\n",
    "    cloudiness.append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging code to check the Dataframes generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prodlist = bestlist\n",
    "len(prodlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(products_list_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_list_sorted[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_list_sorted[0]['link'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_list_sorted[0]['title'][0] #title\n",
    "products_list_sorted[0]['granuleidentifier'][0][13:16] #granuleName\n",
    "products_list_sorted[0]['granuleidentifier'][0][49:55] #tileName\n",
    "products_list_sorted[0]['granuleidentifier'][0][41:48] #granuleName\n",
    "products_list_sorted[1]['datastripidentifier'][0][42:57] #granuleDate\n",
    "products_list_sorted[1]['identifier'][0][11:26] #tileDate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Construction of Download URL from the Dataframe Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def urlConstructor(uuid, title, granuleID, tileID, granuleName, granuleDate, tileDate):\n",
    "    header = \"https://apihub.copernicus.eu/apihub/odata/v1\"\n",
    "    fileName = f\"{tileID}_{tileDate}_TCI_10m.jp2\"\n",
    "    queryUrl = f\"{header}/Products('{uuid}')/Nodes('{title}.SAFE')/Nodes('GRANULE')/Nodes('{granuleID}_{tileID}_{granuleName}_{granuleDate}')/Nodes('IMG_DATA')/Nodes('R10m')/Nodes('{fileName}')/$value\"\n",
    "    thumbUrl = f\"{header}/Products('{uuid}')\"\n",
    "    \n",
    "    return queryUrl, fileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(products_list[1301][products_list[1301]['granuleidentifier'].notna()])\n",
    "clearedProducts_list = []\n",
    "for products in products_list_sorted:\n",
    "    clearedProducts = products[products['granuleidentifier'].notna()]\n",
    "    clearedProducts_list.append(clearedProducts)\n",
    "# products_list_sorted[1301]['granuleidentifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "airportMetaDB = {}\n",
    "qUrls, fNames = [], []\n",
    "for product,iataCode in tqdm(zip(clearedProducts_list,sitelist), total=len(sitelist)):\n",
    "    valueDict = {}\n",
    "    uuid = product['uuid'][0] #3cf16779-b786-462f-8bd5-6fe43a73d213\n",
    "    title = product['title'][0] #S2A_MSIL2A_20211130T162631_N0301_R040_T16SGC_20211130T191654\n",
    "    preview_thumb = product[\"link_icon\"][0] #thumbnail-preview\n",
    "    # print(product['granuleidentifier'][0])\n",
    "    granuleID = product['granuleidentifier'][0][13:16] #L2A\n",
    "    tileID = product['granuleidentifier'][0][49:55] #T50TMK\n",
    "    granuleName = product['granuleidentifier'][0][41:48] #A033642\n",
    "    granuleDate = product['datastripidentifier'][0][42:57] #20211125T030026\n",
    "    tileDate = product['identifier'][0][11:26] #20211125T030029\n",
    "    u = urlConstructor(uuid, title, granuleID, tileID, granuleName, granuleDate, tileDate)[0]\n",
    "    f = urlConstructor(uuid, title, granuleID, tileID, granuleName, granuleDate, tileDate)[1]\n",
    "\n",
    "    \n",
    "    valueDict['uuid'] = uuid\n",
    "    valueDict['title'] = title\n",
    "    valueDict['thumbnail'] = preview_thumb\n",
    "    valueDict['granule_id'] = granuleID\n",
    "    valueDict['tile_id'] = tileID\n",
    "    valueDict['granule_name'] = granuleName\n",
    "    valueDict['granule_date'] = granuleDate\n",
    "    valueDict['tile_date'] = tileDate\n",
    "    valueDict['product_url'] = u\n",
    "    valueDict['product_filename'] = f\n",
    "    valueDict['processed_filename'] = f\"S_{iataCode}_{tileDate[:8]}.tiff\"\n",
    "    \n",
    "    airportMetaDB[iataCode] = valueDict\n",
    "    qUrls.append(u)\n",
    "    fNames.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googURL = []\n",
    "for iatas in list(airportMetaDB.keys()):\n",
    "    currentObj = airportMetaDB[iatas]\n",
    "    header = f'https://storage.googleapis.com/gcp-public-data-sentinel-2/L2/tiles'\n",
    "    folder = f'{currentObj[\"tile_id\"][1:3]}/{currentObj[\"tile_id\"][3:4]}/{currentObj[\"tile_id\"][4:6]}'\n",
    "    product_title = f'{currentObj[\"title\"]}.SAFE'\n",
    "    granule_title = f'{currentObj[\"granule_id\"]}_{currentObj[\"tile_id\"]}_{currentObj[\"granule_name\"]}_{currentObj[\"granule_date\"]}'\n",
    "    file_title = f'{currentObj[\"product_filename\"]}'\n",
    "    \n",
    "    \n",
    "    finalURL = f'{header}/{folder}/{product_title}/GRANULE/{granule_title}/IMG_DATA/R10m/{file_title}'\n",
    "    currentObj[\"goog_url\"] = finalURL    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dumping all data from Query process onto JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "key = datetime.now().strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ForestCoords.json\", \"w\") as outfile:\n",
    "    json.dump(airportMetaDB, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making raw, warped and clipped files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = \"2kmx2km\"\n",
    "current_set = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ForestCoords.json\", \"r\") as infile:\n",
    "    airportDownloadDriver = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fNames = []\n",
    "qUrls = []\n",
    "datestamp = []\n",
    "processedfNames = []\n",
    "\n",
    "for codes in tqdm(sitelist, total = len(sitelist)):\n",
    "    # qUrls.append(airportDownloadDriver[codes][\"product_url\"])\n",
    "#     testing with google URL\n",
    "    qUrls.append(airportDownloadDriver[codes][\"goog_url\"])\n",
    "    fNames.append(airportDownloadDriver[codes][\"product_filename\"])\n",
    "    processedfNames.append(airportDownloadDriver[codes][\"processed_filename\"])\n",
    "    datestamp.append(airportDownloadDriver[codes][\"tile_date\"][:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_directory = f\"../img_data/{batch}/raw/\"\n",
    "os.makedirs(raw_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImages(fNames,urls,iata):\n",
    "    for names,urls,ia in tqdm(zip(fNames,urls,iata),total = len(iata), leave = False):\n",
    "        try:\n",
    "#             Chceking if existing file is valid\n",
    "            productName = f\"{ia}_{names}\"\n",
    "            if productName in os.listdir(raw_directory):\n",
    "                productSize = int(str(os.stat(f\"{raw_directory}/{productName}\").st_size))\n",
    "                if productSize > 1000:\n",
    "                    # print(\"File already exists! skipping\")\n",
    "                    continue\n",
    "            # else:\n",
    "            # print(\"File doesn't exist! Downloading\")\n",
    "            \n",
    "            # r = requests.get(urls,auth = (user,password))\n",
    "            \n",
    "#             Without authentication for google storage\n",
    "            r = requests.get(urls)\n",
    "\n",
    "            # if r.status_code ==200\n",
    "            with open(f\"{raw_directory}/{str(ia)}_{str(names)}\",'wb') as f: \n",
    "                f.write(r.content) \n",
    "            f.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(ia,e)\n",
    "            # print(\"Error Downloading File! Storing Metadata\")\n",
    "            with open(f\"../datafiles/missing/{str(ia)}.json\",'wb') as missingTile:\n",
    "                json.dump(airportDownloadDriver[ia], missingTile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getImages(fNames,qUrls,sitelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "brokenList = []\n",
    "for file in os.listdir(raw_directory):\n",
    "    if os.stat(f\"{raw_directory}/{file}\").st_size < 2048:\n",
    "        brokenList.append(airportDownloadDriver[file[:3]][\"product_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"https://apihub.copernicus.eu/apihub/odata/v1/Products('aedd4c4f-f1d4-45be-8dc6-f649fa34aa5f')/Nodes('S2B_MSIL2A_20220506T021339_N0400_R060_T50HMG_20220506T061534.SAFE')/Nodes('GRANULE')/Nodes('L2A_T50HMG_A026970_20220506T022353')/Nodes('IMG_DATA')/Nodes('R10m')/Nodes('T50HMG_20220506T021339_B02_60m.jp2')/$value\",\n",
       " \"https://apihub.copernicus.eu/apihub/odata/v1/Products('aedd4c4f-f1d4-45be-8dc6-f649fa34aa5f')/Nodes('S2B_MSIL2A_20220506T021339_N0400_R060_T50HMG_20220506T061534.SAFE')/Nodes('GRANULE')/Nodes('L2A_T50HMG_A026970_20220506T022353')/Nodes('IMG_DATA')/Nodes('R10m')/Nodes('T50HMG_20220506T021339_B02_60m.jp2')/$value\"]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brokenList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedFileList = []\n",
    "for ia in tqdm(sitelist):\n",
    "    for files in os.listdir(raw_directory):\n",
    "        newstring = [ia, '_']\n",
    "        x = ''.join(newstring)\n",
    "        if files.endswith(\".jp2\") and x in files:\n",
    "            sortedFileList.append(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataArr = []\n",
    "for items in tqdm(sortedFileList, total=len(sitelist)):\n",
    "    data = rio.open(f\"{raw_directory}/{str(items)}\")\n",
    "    #print(items)\n",
    "    #print(data.meta['crs'])\n",
    "    dataArr.append(data)\n",
    "    data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetPath = f\"../img_data/{batch}/warped/\"\n",
    "os.makedirs(targetPath, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sortedFileList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNames,file = [],[]\n",
    "for items,meta,ia in tqdm(zip(sortedFileList,dataArr,sitelist),total = len(sitelist)):\n",
    "    openFile = f\"{raw_directory}/{str(items)}\"\n",
    "    # saveFileName = str(sitelist.index(str(ia)) + 1) + \"_\"  + str(items)[:-4] + \".tiff\"\n",
    "    saveFileName = str(items)[:-4] + \".tiff\"\n",
    "    saveFile = targetPath + saveFileName\n",
    "    fileNames.append(saveFile)\n",
    "    if saveFileName not in os.listdir(targetPath):\n",
    "    #print(openFile)\n",
    "        input_raster = gdal.Open(openFile)\n",
    "        #print(saveFile)\n",
    "        gdal.Warp(saveFile,\n",
    "                  openFile,\n",
    "                  dstSRS = 'epsg:4326',\n",
    "                  width = str(meta.meta['width']),\n",
    "                  height = str(meta.meta['height'])\n",
    "                 )\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipped_path = f\"../img_data/{batch}/clipped/{current_set}\"\n",
    "os.makedirs(clipped_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoms = []\n",
    "for i in range(0,len(datestamp)):\n",
    "    geoms.append(polygons[i]['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for names,outNames, g,date,ia in tqdm(zip(fileNames, processedfNames,geoms,datestamp,sitelist),total = len(fileNames)):\n",
    "    try:\n",
    "        with rio.open(names) as src:\n",
    "                out_image, out_transform = mask(src, g, crop=True, filled=True)\n",
    "                out_meta = src.meta.copy()\n",
    "\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                             \"height\": out_image.shape[1],\n",
    "                             \"width\": out_image.shape[2],\n",
    "                             \"transform\": out_transform})\n",
    "\n",
    "        with rio.open(f\"{clipped_path}/{outNames}\", \"w\", **out_meta) as dest:\n",
    "            dest.write(out_image)\n",
    "        # os.remove(titlelist[k][i])\n",
    "        # print(ia)\n",
    "        src.close()\n",
    "        dest.close()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "RS-Python.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
