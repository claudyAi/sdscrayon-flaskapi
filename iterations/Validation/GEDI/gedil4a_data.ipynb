{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import requests\n",
    "import datetime as dt \n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import contextily as ctx \n",
    "from shapely.geometry import MultiPolygon, Polygon, box\n",
    "from shapely.ops import orient\n",
    "\n",
    "import pygc\n",
    "import h5py\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import os\n",
    "from os import path\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "from shapely.ops import orient\n",
    "import fiona\n",
    "\n",
    "from csv import writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Search with bounding box\n",
    "NASA EarthData's unique ID for this dataset (called `Concept ID`) is needed for searching the dataset. The dataset Digital Object Identifier or DOI can be used to obtain the `Concept ID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables for data search\n",
    "\n",
    "# downloaded data file paths\n",
    "h5_directory = 'C:/gedi_data'\n",
    "gedi_shp_directory = './gedi_shp'\n",
    "outdir = './subsets'\n",
    "\n",
    "# coordinates file path\n",
    "main_datafile_path = \"estingAustralia.csv\"\n",
    "\n",
    "# lengths in m\n",
    "ew_width = 1000\n",
    "ns_height = 1000\n",
    "size = int(ew_width/1000)\n",
    "\n",
    "start_date = dt.datetime(2022, 1, 1) # specify your own start date\n",
    "end_date = dt.datetime(2022, 6, 1)  # specify your end start date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C2244602422-ORNL_CLOUD\n"
     ]
    }
   ],
   "source": [
    "doi = '10.3334/ORNLDAAC/2017'# GEDI L4A DOI \n",
    "\n",
    "# CMR API base url\n",
    "cmrurl='https://cmr.earthdata.nasa.gov/search/' \n",
    "\n",
    "doisearch = cmrurl + 'collections.json?doi=' + doi\n",
    "response = requests.get(doisearch)\n",
    "response.raise_for_status()\n",
    "concept_id = response.json()['feed']['entry'][0]['id']\n",
    "\n",
    "print(concept_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to obtain bounding box from coordinate\n",
    "def latLonBoxByWandH(lat,lon,ew_width,ns_height,site):\n",
    "    lats, lons = [], []\n",
    "    #distance in m, az (in deg), lat (in deg), long (in deg)\n",
    "\n",
    "    res = pygc.great_circle(distance=ew_width/2, azimuth=90, latitude=lat, longitude=lon)\n",
    "    lat, lon = res['latitude'], res['longitude']\n",
    "\n",
    "    res = pygc.great_circle(distance=ns_height/2, azimuth=180, latitude=lat, longitude=lon)\n",
    "    lat, lon = res['latitude'], res['longitude']\n",
    "    lats.append(lat), lons.append(lon)\n",
    "\n",
    "    res = pygc.great_circle(distance=ew_width, azimuth=270, latitude=lat, longitude=lon)\n",
    "    lat, lon = res['latitude'], res['longitude']\n",
    "    lats.append(lat), lons.append(lon)\n",
    "\n",
    "    res = pygc.great_circle(distance=ns_height, azimuth=0, latitude=lat, longitude=lon)\n",
    "    lat, lon = res['latitude'], res['longitude']\n",
    "    lats.append(lat), lons.append(lon)\n",
    "\n",
    "    res = pygc.great_circle(distance=ew_width, azimuth=90, latitude=lat, longitude=lon)\n",
    "    lat, lon = res['latitude'], res['longitude']\n",
    "    lats.append(lat), lons.append(lon)\n",
    "    \n",
    "    return {'lats':lats,'lons':lons, 'site': site}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>project</th>\n",
       "      <th>site</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>Granule Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>SouthWestForests-DON019FireInv</td>\n",
       "      <td>k_1</td>\n",
       "      <td>-34.7310</td>\n",
       "      <td>116.2081</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>SouthWestForests-DON019FireInv</td>\n",
       "      <td>k_2</td>\n",
       "      <td>-34.7265</td>\n",
       "      <td>116.2081</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>SouthWestForests-DON019FireInv</td>\n",
       "      <td>k_3</td>\n",
       "      <td>-34.6949</td>\n",
       "      <td>116.2085</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>SouthWestForests-DON019FireInv</td>\n",
       "      <td>k_4</td>\n",
       "      <td>-34.7265</td>\n",
       "      <td>116.2136</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>SouthWestForests-DON019FireInv</td>\n",
       "      <td>k_5</td>\n",
       "      <td>-34.7221</td>\n",
       "      <td>116.2136</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>318</td>\n",
       "      <td>LIRE</td>\n",
       "      <td>k_242</td>\n",
       "      <td>-41.3530</td>\n",
       "      <td>147.5222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>319</td>\n",
       "      <td>Ausplot Forest Monitoring Network</td>\n",
       "      <td>k_243</td>\n",
       "      <td>-41.3671</td>\n",
       "      <td>147.6032</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>320</td>\n",
       "      <td>LIPL</td>\n",
       "      <td>k_244</td>\n",
       "      <td>-42.4391</td>\n",
       "      <td>147.7789</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>321</td>\n",
       "      <td>LIPL</td>\n",
       "      <td>k_245</td>\n",
       "      <td>-42.7232</td>\n",
       "      <td>147.8451</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>322</td>\n",
       "      <td>LIMI</td>\n",
       "      <td>k_246</td>\n",
       "      <td>-42.7245</td>\n",
       "      <td>147.8632</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>246 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                            project   site      lat      long  \\\n",
       "0             0     SouthWestForests-DON019FireInv    k_1 -34.7310  116.2081   \n",
       "1             1     SouthWestForests-DON019FireInv    k_2 -34.7265  116.2081   \n",
       "2             2     SouthWestForests-DON019FireInv    k_3 -34.6949  116.2085   \n",
       "3             3     SouthWestForests-DON019FireInv    k_4 -34.7265  116.2136   \n",
       "4             4     SouthWestForests-DON019FireInv    k_5 -34.7221  116.2136   \n",
       "..          ...                                ...    ...      ...       ...   \n",
       "241         318                               LIRE  k_242 -41.3530  147.5222   \n",
       "242         319  Ausplot Forest Monitoring Network  k_243 -41.3671  147.6032   \n",
       "243         320                               LIPL  k_244 -42.4391  147.7789   \n",
       "244         321                               LIPL  k_245 -42.7232  147.8451   \n",
       "245         322                               LIMI  k_246 -42.7245  147.8632   \n",
       "\n",
       "     Granule Number  \n",
       "0                 2  \n",
       "1                 2  \n",
       "2                 2  \n",
       "3                 1  \n",
       "4                 1  \n",
       "..              ...  \n",
       "241               1  \n",
       "242               3  \n",
       "243               3  \n",
       "244               5  \n",
       "245               4  \n",
       "\n",
       "[246 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading coordinates file\n",
    "treecoords = pd.read_csv(main_datafile_path)\n",
    "treecoords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file cleanup\n",
    "def removefiles(directory, ext) :\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "        if ext == \"any\" :\n",
    "            if os.path.isfile(f):\n",
    "                os.remove(f)\n",
    "        else:\n",
    "            if f.endswith(ext):\n",
    "                os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean downloaded files\n",
    "def renamefiles(directory) :\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(f):\n",
    "            pre, ext = os.path.splitext(f)\n",
    "            os.rename(f, pre + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create ESRI shapefile\n",
    "def createshp(dataframe) :\n",
    "    # define schema\n",
    "    schema = {\n",
    "        'geometry':'Polygon',\n",
    "        'properties':[('Name','str')]\n",
    "    }\n",
    "\n",
    "    #open a fiona object\n",
    "    polyShp = fiona.open('gedi_shp/cropcopymark.shp', mode='w', driver='ESRI Shapefile',\n",
    "            schema = schema, crs = \"EPSG:4326\")\n",
    "\n",
    "    #get list of points\n",
    "    xyList = []\n",
    "    rowName = ''\n",
    "    for index, row in dataframe.iterrows():\n",
    "        xyList.append((row.lons,row.lats))\n",
    "        rowName = row.site\n",
    "    xyList[:5]\n",
    "\n",
    "    #save record and close shapefile\n",
    "    rowDict = {\n",
    "    'geometry' : {'type':'Polygon',\n",
    "                    'coordinates': [xyList]}, #Here the xyList is in brackets\n",
    "    'properties': {'Name' : rowName},\n",
    "    }\n",
    "    polyShp.write(rowDict)\n",
    "    #close fiona object\n",
    "    polyShp.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to look for dataset with polygon\n",
    "def lookdata(grsm_poly):\n",
    "    # CMR formatted start and end times\n",
    "    dt_format = '%Y-%m-%dT%H:%M:%SZ'\n",
    "    temporal_str = start_date.strftime(dt_format) + ',' + end_date.strftime(dt_format)\n",
    "    \n",
    "    # converting to WGS84 coordinate system\n",
    "    grsm_epsg4326 = grsm_poly.to_crs(epsg=4326)\n",
    "\n",
    "    # orienting coordinates clockwise\n",
    "    grsm_epsg4326.geometry = grsm_epsg4326.geometry.apply(orient, args=(1,))\n",
    "\n",
    "    # reducing number of vertices in the polygon\n",
    "    # CMR has 1000000 bytes limit\n",
    "    grsm_epsg4326 = grsm_epsg4326.simplify(0.0005)\n",
    "\n",
    "    geojson = {\"shapefile\": (\"grsm.json\", grsm_epsg4326.geometry.to_json(), \"application/geo+json\")}\n",
    "\n",
    "    page_num = 1\n",
    "    page_size = 2000 # CMR page size limit\n",
    "\n",
    "    granule_arr = []\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        # defining parameters\n",
    "        cmr_param = {\n",
    "            \"collection_concept_id\": concept_id, \n",
    "            \"page_size\": page_size,\n",
    "            \"page_num\": page_num,\n",
    "            \"temporal\": temporal_str,\n",
    "            \"simplify-shapefile\": 'true' # this is needed to bypass 5000 coordinates limit of CMR\n",
    "        }\n",
    "        \n",
    "        granulesearch = cmrurl + 'granules.json'\n",
    "        response = requests.post(granulesearch, data=cmr_param, files=geojson)\n",
    "        granules = response.json()['feed']['entry']\n",
    "        \n",
    "        if granules:\n",
    "            for g in granules:\n",
    "                granule_url = ''\n",
    "                granule_poly = ''\n",
    "                \n",
    "                # read file size\n",
    "                granule_size = float(g['granule_size'])\n",
    "                \n",
    "                # reading bounding geometries\n",
    "                if 'polygons' in g:\n",
    "                    polygons= g['polygons']\n",
    "                    multipolygons = []\n",
    "                    for poly in polygons:\n",
    "                        i=iter(poly[0].split(\" \"))\n",
    "                        ltln = list(map(\" \".join,zip(i,i)))\n",
    "                        multipolygons.append(Polygon([[float(p.split(\" \")[1]), float(p.split(\" \")[0])] for p in ltln]))\n",
    "                    granule_poly = MultiPolygon(multipolygons)\n",
    "                \n",
    "                # Get URL of HDF5 files\n",
    "                for links in g['links']:\n",
    "                    if 'title' in links and links['title'].startswith('Download') \\\n",
    "                    and links['title'].endswith('.h5'):\n",
    "                        granule_url = links['href']\n",
    "                granule_arr.append([granule_url, granule_size, granule_poly])\n",
    "                \n",
    "            page_num += 1\n",
    "        else: \n",
    "            break\n",
    "\n",
    "    # adding bound as the last row into the dataframe\n",
    "    # we will use this later in the plot\n",
    "    granule_arr.append(['GRSM', 0, grsm_epsg4326.geometry.item() ]) \n",
    "\n",
    "    # creating a pandas dataframe\n",
    "    l4adf = pd.DataFrame(granule_arr, columns=[\"granule_url\", \"granule_size\", \"granule_poly\"])\n",
    "\n",
    "    # Drop granules with empty geometry\n",
    "    l4adf = l4adf[l4adf['granule_poly'] != '']\n",
    "    return l4adf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to loop through all downloaded files and create clipped versions\n",
    "def clipping(indir, outdir, grsm_poly) :\n",
    "    # converting to WGS84 coordinate system\n",
    "    grsm_epsg4326 = grsm_poly.to_crs(epsg=4326)\n",
    "\n",
    "    for infile in glob(path.join(indir, 'GEDI04_A*.h5')):\n",
    "        name, ext = path.splitext(path.basename(infile))\n",
    "        subfilename = \"{name}_sub{ext}\".format(name=name, ext=ext)\n",
    "        outfile = path.join(outdir, path.basename(subfilename))\n",
    "        hf_in = h5py.File(infile, 'r')\n",
    "        hf_out = h5py.File(outfile, 'w')\n",
    "        \n",
    "        # copy ANCILLARY and METADATA groups\n",
    "        var1 = [\"/ANCILLARY\", \"/METADATA\"]\n",
    "        for v in var1:\n",
    "            hf_in.copy(hf_in[v],hf_out)\n",
    "        \n",
    "        # loop through BEAMXXXX groups\n",
    "        for v in list(hf_in.keys()):\n",
    "            if v.startswith('BEAM'):\n",
    "                beam = hf_in[v]\n",
    "                # find the shots that overlays the area of interest (GRSM)\n",
    "                lat = beam['lat_lowestmode'][:]\n",
    "                lon = beam['lon_lowestmode'][:]\n",
    "                i = np.arange(0, len(lat), 1) # index\n",
    "                geo_arr = list(zip(lat,lon, i))\n",
    "                l4adf = pd.DataFrame(geo_arr, columns=[\"lat_lowestmode\", \"lon_lowestmode\", \"i\"])\n",
    "                l4agdf = gpd.GeoDataFrame(l4adf, geometry=gpd.points_from_xy(l4adf.lon_lowestmode, l4adf.lat_lowestmode))\n",
    "                l4agdf.crs = \"EPSG:4326\"\n",
    "                l4agdf_gsrm = l4agdf[l4agdf['geometry'].within(grsm_epsg4326.geometry[0])]  \n",
    "                indices = l4agdf_gsrm.i\n",
    "\n",
    "                # copy BEAMS to the output file\n",
    "                for key, value in beam.items():\n",
    "                    if isinstance(value, h5py.Group):\n",
    "                        for key2, value2 in value.items():\n",
    "                            group_path = value2.parent.name\n",
    "                            group_id = hf_out.require_group(group_path)\n",
    "                            dataset_path = group_path + '/' + key2\n",
    "                            hf_out.create_dataset(dataset_path, data=value2[:][indices])\n",
    "                            for attr in value2.attrs.keys():\n",
    "                                hf_out[dataset_path].attrs[attr] = value2.attrs[attr]\n",
    "                    else:\n",
    "                        group_path = value.parent.name\n",
    "                        group_id = hf_out.require_group(group_path)\n",
    "                        dataset_path = group_path + '/' + key\n",
    "                        hf_out.create_dataset(dataset_path, data=value[:][indices])\n",
    "                        for attr in value.attrs.keys():\n",
    "                            hf_out[dataset_path].attrs[attr] = value.attrs[attr]\n",
    "\n",
    "        hf_in.close()\n",
    "        hf_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(outdir) :\n",
    "    lat_l = []\n",
    "    lon_l = []\n",
    "    agbd = []\n",
    "    for subfile in glob(path.join(outdir, 'GEDI04_A*.h5')):\n",
    "        hf_in = h5py.File(subfile, 'r')\n",
    "        for v in list(hf_in.keys()):\n",
    "            if v.startswith('BEAM'):\n",
    "                beam = hf_in[v]\n",
    "                lat_l.extend(beam['lat_lowestmode'][:].tolist()) \n",
    "                lon_l.extend(beam['lon_lowestmode'][:].tolist()) \n",
    "                agbd.extend(beam['agbd'][:].tolist())\n",
    "        hf_in.close()\n",
    "    geo_arr = list(zip(agbd,lat_l,lon_l))\n",
    "    df = pd.DataFrame(geo_arr, columns=[\"agbd\", \"lat_lowestmode\", \"lon_lowestmode\"])\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.lon_lowestmode, df.lat_lowestmode))\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append to csv file: site, lat, long, abgdavg\n",
    "def updatefile(site, lat, lon, agbd_avg) :\n",
    "    List = [site, lat, lon, agbd_avg]\n",
    "    with open('subsets/grsm_subset.csv', 'a') as f_object:\n",
    "    \n",
    "        # Pass this file object to csv.writer()\n",
    "        # and get a writer object\n",
    "        writer_object = writer(f_object)\n",
    "    \n",
    "        # Pass the list as an argument into\n",
    "        # the writerow()\n",
    "        writer_object.writerow(List)\n",
    "    \n",
    "        # Close the file object\n",
    "        f_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No URLs found in granules.txt.\n"
     ]
    },
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Cannot create a file when that file already exists: 'C:/gedi_data\\\\GEDI04_A_2022019203138_O17586_04_T07726_02_002_02_V002.h5@A-userid=yeowanli&Expires=1698599946&Signature=tTgXxByw4xBfm-uw~rhLQRb6MudBR2LnYMNW1ytQ-Rd8lKfDeddOPS6nVq3BHRzC4igN5lH-RnVRXwzuRVmMu6DvqDH8XECxN5A9ObybsRsa0yii14OCsz1AgzpKDOiF-qhCh8qb' -> 'C:/gedi_data\\\\GEDI04_A_2022019203138_O17586_04_T07726_02_002_02_V002.h5'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\earth\\Downloads\\Workspace\\SDS\\DAI_SatSensing_Workshop\\GEDI\\gedi_data.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/earth/Downloads/Workspace/SDS/DAI_SatSensing_Workshop/GEDI/gedi_data.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mwget.exe --load-cookies .urs_cookies --save-cookies .urs_cookies --keep-session-cookies -P /gedi_data  -nc --content-disposition --trust-server-names -i granules.txt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/earth/Downloads/Workspace/SDS/DAI_SatSensing_Workshop/GEDI/gedi_data.ipynb#X20sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# rename downloaded files\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/earth/Downloads/Workspace/SDS/DAI_SatSensing_Workshop/GEDI/gedi_data.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m renamefiles(h5_directory)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/earth/Downloads/Workspace/SDS/DAI_SatSensing_Workshop/GEDI/gedi_data.ipynb#X20sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# loop through downloaded h5 files and create clipped versions\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/earth/Downloads/Workspace/SDS/DAI_SatSensing_Workshop/GEDI/gedi_data.ipynb#X20sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mif\u001b[39;00m (os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(site)) :\n",
      "\u001b[1;32mc:\\Users\\earth\\Downloads\\Workspace\\SDS\\DAI_SatSensing_Workshop\\GEDI\\gedi_data.ipynb Cell 14\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/earth/Downloads/Workspace/SDS/DAI_SatSensing_Workshop/GEDI/gedi_data.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(f):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/earth/Downloads/Workspace/SDS/DAI_SatSensing_Workshop/GEDI/gedi_data.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     pre, ext \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplitext(f)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/earth/Downloads/Workspace/SDS/DAI_SatSensing_Workshop/GEDI/gedi_data.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     os\u001b[39m.\u001b[39;49mrename(f, pre \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m.h5\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Cannot create a file when that file already exists: 'C:/gedi_data\\\\GEDI04_A_2022019203138_O17586_04_T07726_02_002_02_V002.h5@A-userid=yeowanli&Expires=1698599946&Signature=tTgXxByw4xBfm-uw~rhLQRb6MudBR2LnYMNW1ytQ-Rd8lKfDeddOPS6nVq3BHRzC4igN5lH-RnVRXwzuRVmMu6DvqDH8XECxN5A9ObybsRsa0yii14OCsz1AgzpKDOiF-qhCh8qb' -> 'C:/gedi_data\\\\GEDI04_A_2022019203138_O17586_04_T07726_02_002_02_V002.h5'"
     ]
    }
   ],
   "source": [
    "# subset_df = create_dataframe()\n",
    "for index, row in treecoords.iterrows():\n",
    "    # extract important values for ESRI shapefile\n",
    "    lat = row['lat']\n",
    "    lon = row['long']\n",
    "    site = row['site']\n",
    "    bbox = latLonBoxByWandH(lat,lon,ew_width,ns_height,site)\n",
    "    tmp = pd.DataFrame.from_dict(bbox, orient='columns', dtype=None, columns=None)\n",
    "    \n",
    "    # clean up gedi_shp file directory\n",
    "    removefiles(gedi_shp_directory, \"any\")\n",
    "    removefiles(h5_directory, \".h5\")\n",
    "    \n",
    "    # create ESRI shapefile\n",
    "    createshp(tmp)\n",
    "    \n",
    "    # look for matching data with polygon\n",
    "    grsm_poly = gpd.read_file('gedi_shp/cropcopymark.shp')\n",
    "    l4adf = lookdata(grsm_poly)\n",
    "    \n",
    "    # drop duplicate URLs if any\n",
    "    l4a_granules = l4adf[:-1].drop_duplicates(subset=['granule_url'])\n",
    "    l4a_granules.to_csv('granules.txt', columns = ['granule_url'], index=False, header = False)\n",
    "    \n",
    "    # clean up h5 file directory\n",
    "    removefiles(h5_directory, \".h5\")\n",
    "    \n",
    "    # download data\n",
    "    !wget.exe --load-cookies .urs_cookies --save-cookies .urs_cookies --keep-session-cookies -P /gedi_data  -nc --content-disposition --trust-server-names -i granules.txt\n",
    "    \n",
    "    # rename downloaded files\n",
    "    renamefiles(h5_directory)\n",
    "    \n",
    "    # loop through downloaded h5 files and create clipped versions\n",
    "    if (os.path.isdir(site)) :\n",
    "        pass\n",
    "    else:\n",
    "        newpath = os.path.join(outdir, site)\n",
    "        os.mkdir(newpath)\n",
    "    clipping(h5_directory,newpath, grsm_poly)\n",
    "    \n",
    "    # create dataframe of relevant GEDI points from clipped versions\n",
    "    # subset_df = create_dataframe(outdir)\n",
    "    \n",
    "    # clear directories\n",
    "    removefiles(h5_directory, \".h5\")\n",
    "    removefiles(gedi_shp_directory, \"any\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
